"""
Analyseur vid√©o pour DeepGuard
D√©tection de deepfakes frame par frame
"""

import cv2
import numpy as np
from PIL import Image
import torch
from pathlib import Path
import logging
from typing import Dict, List, Optional, Tuple
import time
from facenet_pytorch import MTCNN

logger = logging.getLogger(__name__)


class VideoDeepfakeAnalyzer:
    """
    Analyseur de vid√©os pour d√©tection de deepfakes
    
    Processus :
    1. Extraction de frames √©chantillonn√©es
    2. D√©tection de visages (MTCNN)
    3. Pr√©diction sur chaque visage
    4. Agr√©gation des r√©sultats
    """
    
    def __init__(self, deepfake_predictor, device='cpu'):
        """
        Args:
            deepfake_predictor: Instance de DeepfakeDetectorV3
            device: 'cpu' ou 'cuda'
        """
        self.predictor = deepfake_predictor
        self.device = torch.device(device)
        
        # MTCNN pour d√©tection de visages
        self.face_detector = MTCNN(
            image_size=224,
            margin=20,
            min_face_size=40,
            thresholds=[0.6, 0.7, 0.7],
            factor=0.709,
            post_process=True,
            keep_all=False,  # Garder le visage le plus confiant
            device=self.device
        )
        
        logger.info(f"‚úÖ VideoAnalyzer initialis√© sur {self.device}")

    def extract_frames(self, video_path: str, max_frames: int = 30, sample_rate: int = 10) -> List[Tuple[np.ndarray, int]]:
        """
        Extrait des frames d'une vid√©o
        
        Args:
            video_path: chemin vers la vid√©o
            max_frames: nombre maximum de frames √† extraire
            sample_rate: extraire 1 frame toutes les N frames
            
        Returns:
            Liste de (frame_array, frame_index)
        """
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"Impossible d'ouvrir la vid√©o : {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        duration = total_frames / fps if fps > 0 else 0
        
        logger.info(f"üìπ Vid√©o : {total_frames} frames, {fps:.1f} FPS, {duration:.1f}s")
        
        frames = []
        frame_idx = 0
        
        # Calculer les indices de frames √† extraire
        if total_frames <= max_frames * sample_rate:
            # Vid√©o courte : prendre toutes les N frames
            target_indices = list(range(0, total_frames, sample_rate))
        else:
            # Vid√©o longue : √©chantillonner uniform√©ment
            target_indices = np.linspace(0, total_frames - 1, max_frames, dtype=int)
        
        target_indices = target_indices[:max_frames]
        
        for target_idx in target_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, target_idx)
            ret, frame = cap.read()
            
            if ret:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append((frame_rgb, target_idx))
        
        cap.release()
        
        logger.info(f"‚úÖ {len(frames)} frames extraites sur {total_frames}")
        return frames

    def detect_faces_in_frames(self, frames: List[Tuple[np.ndarray, int]]) -> List[Dict]:
        """
        D√©tecte les visages dans les frames
        
        Args:
            frames: liste de (frame_array, frame_index)
            
        Returns:
            Liste de r√©sultats avec visages d√©tect√©s
        """
        results = []
        
        for frame_rgb, frame_idx in frames:
            try:
                pil_img = Image.fromarray(frame_rgb)
                
                # D√©tection de visage avec MTCNN
                face_tensor = self.face_detector(pil_img)
                
                if face_tensor is not None:
                    # Convertir tensor vers PIL Image
                    face_array = face_tensor.permute(1, 2, 0).cpu().numpy()
                    face_array = (face_array * 255).astype(np.uint8)
                    face_pil = Image.fromarray(face_array)
                    
                    results.append({
                        'frame_index': frame_idx,
                        'face_detected': True,
                        'face_image': face_pil,
                        'original_frame': frame_rgb
                    })
                else:
                    results.append({
                        'frame_index': frame_idx,
                        'face_detected': False,
                        'face_image': None,
                        'original_frame': frame_rgb
                    })
                    
            except Exception as e:
                logger.warning(f"Erreur d√©tection visage frame {frame_idx}: {e}")
                results.append({
                    'frame_index': frame_idx,
                    'face_detected': False,
                    'face_image': None,
                    'original_frame': frame_rgb,
                    'error': str(e)
                })
        
        faces_detected = sum(1 for r in results if r['face_detected'])
        logger.info(f"üë§ Visages d√©tect√©s : {faces_detected}/{len(results)} frames")
        
        return results

    def analyze_video(self, video_path: str, max_frames: int = 30, sample_rate: int = 10) -> Dict:
        """
        Analyse compl√®te d'une vid√©o
        
        Args:
            video_path: chemin vers la vid√©o
            max_frames: nombre max de frames √† analyser
            sample_rate: √©chantillonnage des frames
            
        Returns:
            Dict avec r√©sultats d'analyse
        """
        start_time = time.time()
        
        logger.info(f"üé¨ Analyse vid√©o : {video_path}")
        
        try:
            # 1. Extraction de frames
            frames = self.extract_frames(video_path, max_frames, sample_rate)
            
            if not frames:
                return {
                    'success': False,
                    'error': 'Aucune frame extraite',
                    'video_path': video_path
                }
            
            # 2. D√©tection de visages
            face_results = self.detect_faces_in_frames(frames)
            
            # 3. Pr√©dictions sur les visages
            predictions = []
            valid_faces = 0
            
            for result in face_results:
                if result['face_detected'] and result['face_image']:
                    try:
                        pred = self.predictor.predict(result['face_image'])
                        predictions.append({
                            'frame_index': result['frame_index'],
                            'prediction': pred,
                            'is_fake': pred['is_deepfake'],
                            'confidence': pred['confidence']
                        })
                        valid_faces += 1
                    except Exception as e:
                        logger.warning(f"Erreur pr√©diction frame {result['frame_index']}: {e}")
            
            if not predictions:
                return {
                    'success': False,
                    'error': 'Aucun visage analysable trouv√©',
                    'video_path': video_path,
                    'frames_extracted': len(frames),
                    'faces_detected': sum(1 for r in face_results if r['face_detected'])
                }
            
            # 4. Agr√©gation des r√©sultats
            fake_predictions = [p for p in predictions if p['is_fake']]
            real_predictions = [p for p in predictions if not p['is_fake']]
            
            fake_count = len(fake_predictions)
            real_count = len(real_predictions)
            total_predictions = len(predictions)
            
            # D√©cision finale : majorit√© pond√©r√©e par la confiance
            fake_confidence_sum = sum(p['confidence'] for p in fake_predictions)
            real_confidence_sum = sum(p['confidence'] for p in real_predictions)
            
            # Score global
            if fake_count > 0 and real_count > 0:
                # Pond√©ration par confiance moyenne
                fake_weighted = fake_confidence_sum / fake_count if fake_count > 0 else 0
                real_weighted = real_confidence_sum / real_count if real_count > 0 else 0
                
                # D√©cision bas√©e sur confiance pond√©r√©e + majorit√©
                fake_score = (fake_count / total_predictions) * fake_weighted
                real_score = (real_count / total_predictions) * real_weighted
                
                is_deepfake = fake_score > real_score
                confidence = max(fake_score, real_score)
            else:
                # Cas simple : toutes les pr√©dictions vont dans le m√™me sens
                is_deepfake = fake_count > real_count
                confidence = np.mean([p['confidence'] for p in predictions])
            
            processing_time = time.time() - start_time
            
            result = {
                'success': True,
                'video_path': video_path,
                'is_deepfake': bool(is_deepfake),
                'prediction': 'FAKE' if is_deepfake else 'REAL',
                'confidence': float(confidence),  # ‚Üê Conversion explicite
                'processing_time_seconds': float(processing_time),  # ‚Üê Conversion explicite
                'analysis_details': {
                    'frames_extracted': int(len(frames)),  # ‚Üê Conversion explicite
                    'faces_detected': int(sum(1 for r in face_results if r['face_detected'])),  # ‚Üê Conversion explicite
                    'faces_analyzed': int(total_predictions),  # ‚Üê Conversion explicite
                    'fake_predictions': int(fake_count),  # ‚Üê Conversion explicite
                    'real_predictions': int(real_count),  # ‚Üê Conversion explicite
                    'fake_percentage': float((fake_count / total_predictions) * 100) if total_predictions > 0 else 0.0,  # ‚Üê Conversion explicite
                    'avg_confidence': float(np.mean([p['confidence'] for p in predictions]))  # ‚Üê Conversion explicite
                },
                'frame_predictions': [
                    {
                        'frame_index': int(p['frame_index']),  # ‚Üê Conversion explicite
                        'is_fake': bool(p['is_fake']),  # ‚Üê Conversion explicite
                        'confidence': float(p['confidence'])  # ‚Üê Conversion explicite
                    }
                    for p in predictions[:10]  # Garder max 10 d√©tails pour l'API
                ]
            }
            
            logger.info(f"‚úÖ Analyse termin√©e : {result['prediction']} ({float(confidence)*100:.1f}%)")
            logger.info(f"   {total_predictions} faces analys√©es en {processing_time:.1f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse vid√©o : {e}")
            return {
                'success': False,
                'error': str(e),
                'video_path': str(video_path),  # ‚Üê Conversion explicite
                'processing_time_seconds': float(time.time() - start_time)  # ‚Üê Conversion explicite
            }

def create_video_analyzer(predictor, device='cpu') -> VideoDeepfakeAnalyzer:
    """
    Factory function pour cr√©er un analyseur vid√©o
    
    Args:
        predictor: Instance de DeepfakeDetectorV3
        device: 'cpu' ou 'cuda'
        
    Returns:
        Instance de VideoDeepfakeAnalyzer
    """
    return VideoDeepfakeAnalyzer(predictor, device)